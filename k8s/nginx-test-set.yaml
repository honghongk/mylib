# 스토리지 클래스 로컬 프로비저너로 연결
# https://github.com/rancher/local-path-provisioner

apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-nginx
  namespace: test
data:
  # k => v 로만 가능
  # 파일 => 내용
  nginx.conf: |-
    server {
        listen          80;
        server_name    localhost;

        charset utf-8;
        access_log /var/log/nginx/access_log main;

        location / {
            root    /var/www/html;
            index   index.html index.php;
        }
        # error_page 404 /404.html;
        # error_page 500 502 503 504 /50x.html
        location = /50x.html {
            root /usr/share/nginx/html;
        }

        # proxy php
        # location ~ \.php${
        #     proxy_pass http://127.0.0.1;
        # }

        # interpreter
        # location ~ \.php$ {
        #     root            html;
        #     fastcgi_pass    127.0.0.1:9000;
        #     fastcgi_index   index.php;
        #     fastcgi_param   SCRIPT_FILENAME /scripts$fastcgi_script_name;
        #     include         fastcgi_params;
        # }

        location ~ /\.ht {
            deny all;
        }
    }
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
  namespace: test
spec:
  # 위의 링크에 스토리지 클래스 이름
  storageClassName: local-path
  accessModes:
  - ReadWriteOnce
  # local-path-storage 이거는 ReadWriteOnce만 가능
  # - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
# deployment로 만들어내기
# kubectl expose deployment nginx-deployment -n test --port=88 --target-port=80 --type=LoadBalancer
# 노드포트나 IP 쓰는 중인지는 netstat 으로 확인 못함
# svclb pod가 만들어짐
apiVersion: v1
kind: Service
metadata:
  finalizers:
  - service.kubernetes.io/load-balancer-cleanup
  labels:
    app: nginx
  name: nginx-deployment
  namespace: test
spec:
  allocateLoadBalancerNodePorts: true
  # 랜덤생성됨
  # clusterIP: 10.43.236.1
  # clusterIPs:
  # - 10.43.236.1
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - # 호스트 포트
    port: 88
    protocol: TCP
    # 컨테이너 포트
    targetPort: 80
    # 랜덤생성됨
    # nodePort: 31437
  selector:
    app: nginx
  # 클라이언트가 매번 같은 컨테이너로 가는지 등 설정
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    # 일단 ifconfig의 eth0으로만 해봤음
    - ip: 0.0.0.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: test
  labels:
    app: nginx
spec:
  minReadySeconds: 5
  # deploy 대기 시간 후 실패처리
  progressDeadlineSeconds: 600
  # pod 수
  replicas: 5
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx
  strategy:
    type: rollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  # pod 와 똑같음
  template:
    metadata:
      # pod 라벨 지정
      labels:
        app: nginx
    spec:
      # 파일로 configmap 전달하기
      # 볼륨 정의하고 container에서 마운트
      volumes:
      - name: config
        configMap:
          name: cm-nginx
      - name: pvc
        persistentVolumeClaim:
          claimName: test-pvc
      #   # 아직뭔지 잘모름
      #   hostPath:
      #     path: /etc/nginx/conf.d
      #     type: DirectoryOrCreate
      containers:
      - name: nginx
        image: nginx:1.14.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
          protocol: TCP
        volumeMounts:
        # 위의 볼륨이름을 가져와야함
        - name: config
          mountPath: /etc/nginx/conf.d
          readOnly: true
        - name: pvc
          mountPath: /var/www/html

        # 환경변수로 configmap 전달하기
        # env:
        # - name: LANGUAGE
        #   valueFrom:
        #     configMapKeyRef:
        #       name: cm-name
        #       key: language
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        resources:
          limits:
            memory: 64Mi
            cpu: 500m
            # ephemeral-storage: "2Gi"
          requests:
            memory: 64Mi
            cpu: 500m
            # ephemeral-storage: "2Gi"
        
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
---
# 명령어로 생성한거를 yaml으로 뺏음
# kubectl autoscale deployment/nginx-deployment --min=2 --max=15 --cpu-percent=80 -n test

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-deployment
  namespace: test
spec:
  maxReplicas: 15
  minReplicas: 3
  metrics:
  - resource:
      name: cpu
      target:
        # 사용량에 따라 replica 조절 / 퍼센트인듯
        averageUtilization: 80
        type: Utilization
    type: Resource
  # 오토스케일 타겟
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
status:
  conditions:
  - message: recommended size matches current size
    reason: ReadyForNewScale
    status: "True"
    type: AbleToScale
  - message: the HPA was able to successfully calculate a replica count from cpu resource
      utilization (percentage of request)
    reason: ValidMetricFound
    status: "True"
    type: ScalingActive
  - message: the desired replica count is less than the minimum replica count
    reason: TooFewReplicas
    status: "True"
    type: ScalingLimited
  currentMetrics:
  - resource:
      current:
        averageUtilization: 0
        averageValue: "0"
      name: cpu
    type: Resource
  currentReplicas: 2
  desiredReplicas: 2
